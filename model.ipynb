{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhjj/anaconda2/envs/python3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self,word_embedding):\n",
    "        self.paragraph_encoding_hidden_size=128\n",
    "        self.paragraph_encoding_layer_num=3\n",
    "        self.dropout=0.3\n",
    "        self.question_encoding_hidden_size=128\n",
    "        self.question_encoding_layer_num=3\n",
    "        self.word_embedding=word_embedding\n",
    "        self.batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocReader():\n",
    "    def __init__(self,config):\n",
    "        self.config=config\n",
    "        self.add_placeholder()\n",
    "        self.add_word_embedding()\n",
    "        self.logic=self.get_logic()\n",
    "        self.loss=self.get_loss(self.logic[0],self.logic[1])\n",
    "        self.train=self.train(self.loss)\n",
    "        self.predict=self.predict()\n",
    "        \n",
    "    def add_placeholder(self):\n",
    "        self.paragraph=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size,None])\n",
    "        self.paragraph_length=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "        self.paragraph_mask=tf.placeholder(dtype=tf.float32,shape=[self.config.batch_size,None])\n",
    "        self.paragraph_features=tf.placeholder(dtype=tf.float32,shape=[self.config.batch_size,None,51])\n",
    "        self.question=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size,None])\n",
    "        self.question_length=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "        self.question_mask=tf.placeholder(dtype=tf.float32,shape=[self.config.batch_size,None])\n",
    "        self.start_index=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "        self.end_index=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "#         self.paragraph=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size,52])\n",
    "#         self.paragraph_length=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "#         self.paragraph_mask=tf.placeholder(dtype=tf.float32,shape=[self.config.batch_size,52])\n",
    "#         self.paragraph_features=tf.placeholder(dtype=tf.float32,shape=[self.config.batch_size,52,51])\n",
    "#         self.question=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size,9])\n",
    "#         self.question_length=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "#         self.question_mask=tf.placeholder(dtype=tf.float32,shape=[self.config.batch_size,9])\n",
    "#         self.start_index=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "#         self.end_index=tf.placeholder(dtype=tf.int32,shape=[self.config.batch_size])\n",
    "                \n",
    "\n",
    "\n",
    "    \n",
    "    def add_word_embedding(self):\n",
    "        self.word_embedding=tf.get_variable('word_embedding',initializer=self.config.word_embedding)\n",
    "        \n",
    "        \n",
    "    def get_logic(self):\n",
    "        p_encoding=self.paragraph_encoding()\n",
    "#         print(p_encoding.shape)\n",
    "        q_encoding=self.question_encoding()\n",
    "#         print(q_encoding.shape)\n",
    "        with tf.variable_scope('start'):\n",
    "            start_scores=self.biliner_attention(p_encoding,q_encoding,self.paragraph_mask)\n",
    "        with tf.variable_scope('end'):\n",
    "            end_scores=self.biliner_attention(p_encoding,q_encoding,self.paragraph_mask)\n",
    "        return start_scores,end_scores\n",
    "        \n",
    "        \n",
    "    def get_loss(self,start_scores,end_scores):\n",
    "        loss=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.start_index,logits=start_scores)+\\\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.end_index,logits=end_scores)\n",
    "        loss=tf.reduce_mean(loss)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def train(self,loss):\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        train_op=optimizer.minimize(loss)\n",
    "        return train_op\n",
    "    \n",
    "    \n",
    "    def paragraph_encoding(self):\n",
    "        with tf.variable_scope('paragraph_encoding'):\n",
    "            def get_cell():\n",
    "                cell=tf.nn.rnn_cell.BasicLSTMCell(self.config.paragraph_encoding_hidden_size)\n",
    "                cell=tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=self.config.dropout)\n",
    "                return cell\n",
    "#             cell=tf.nn.rnn_cell.BasicLSTMCell(self.config.paragraph_encoding_hidden_size)\n",
    "#             cell_1=tf.nn.rnn_cell.BasicLSTMCell(self.config.paragraph_encoding_hidden_size)\n",
    "#             cell_2=tf.nn.rnn_cell.BasicLSTMCell(self.config.paragraph_encoding_hidden_size)\n",
    "    #         cell=tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=self.config.dropout)\n",
    "            cell_fw=tf.nn.rnn_cell.MultiRNNCell([get_cell() for _ in range(self.config.paragraph_encoding_layer_num)])\n",
    "            cell_bw=tf.nn.rnn_cell.MultiRNNCell([get_cell() for _ in range(self.config.paragraph_encoding_layer_num)])\n",
    "            paragraph=tf.nn.embedding_lookup(self.word_embedding,self.paragraph)\n",
    "#             print(paragraph.shape)\n",
    "            p_es=[paragraph]\n",
    "            p_es.append(self.paragraph_features)\n",
    "#             print(self.paragraph_features.shape)\n",
    "            p_q_scores=self.p_q_aligned()\n",
    "#             print(p_q_scores.shape)\n",
    "            p_es.append(p_q_scores)\n",
    "            paragraph=tf.concat(p_es,-1)\n",
    "#             print(paragraph.shape)\n",
    "            p_encoding,_=tf.nn.bidirectional_dynamic_rnn(cell_fw,cell_bw,paragraph,sequence_length=self.paragraph_length,dtype=tf.float32)\n",
    "#             p_encoding,_=tf.nn.bidirectional_dynamic_rnn(cell_1,cell_2,paragraph,sequence_length=self.paragraph_length,dtype=tf.float32)\n",
    "\n",
    "            p_encoding=tf.concat(p_encoding,-1)\n",
    "            \n",
    "           \n",
    "            return p_encoding\n",
    "    \n",
    "    def question_encoding(self):\n",
    "        with tf.variable_scope('question_encoding'):\n",
    "            def get_cell():\n",
    "                cell=tf.nn.rnn_cell.BasicLSTMCell(self.config.question_encoding_hidden_size)\n",
    "                cell=tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=self.config.dropout)\n",
    "                return cell\n",
    "#             cell_1=tf.nn.rnn_cell.BasicLSTMCell(self.config.question_encoding_hidden_size)\n",
    "#             cell_2=tf.nn.rnn_cell.BasicLSTMCell(self.config.question_encoding_hidden_size)\n",
    "#         cell=tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=self.config.dropout)\n",
    "            cell_fw=tf.nn.rnn_cell.MultiRNNCell([get_cell() for _ in range(self.config.question_encoding_layer_num)])\n",
    "            cell_bw=tf.nn.rnn_cell.MultiRNNCell([get_cell() for _ in range(self.config.question_encoding_layer_num)])\n",
    "            question=tf.nn.embedding_lookup(self.word_embedding,self.question)\n",
    "            q_encoding,_=tf.nn.bidirectional_dynamic_rnn(cell_fw,cell_bw,question,sequence_length=self.question_length,dtype=tf.float32)\n",
    "#             q_encoding,_=tf.nn.bidirectional_dynamic_rnn(cell_1,cell_2,question,sequence_length=self.question_length,dtype=tf.float32)\n",
    "            q_encoding=tf.concat(q_encoding,-1)\n",
    "            alpha=self.liner_attention(q_encoding,self.question_mask)\n",
    "            alpha=tf.concat([tf.expand_dims(alpha,-1)]*self.config.question_encoding_hidden_size*2,-1)\n",
    "            q_encoding=tf.reduce_sum(tf.multiply(q_encoding,alpha),axis=1)\n",
    "            return q_encoding\n",
    "    \n",
    "    def liner_attention(self,x,x_mask):\n",
    "        w=tf.get_variable('liner_attention_w',shape=[x.shape[-1],1])\n",
    "        scores=tf.matmul(tf.reshape(x,(-1,x.shape[-1])),w)\n",
    "        scores=tf.add(tf.reshape(scores,(x.shape[0],-1)),x_mask)\n",
    "        alpha=tf.nn.softmax(scores)\n",
    "        return alpha\n",
    "    \n",
    "    def biliner_attention(self,x,y,x_mask):\n",
    "        w=tf.get_variable('biliner_attention_w',shape=[y.shape[-1],x.shape[-1]])\n",
    "        yw=tf.matmul(y,w)\n",
    "        yw=tf.expand_dims(yw,axis=-1)\n",
    "        scores=tf.matmul(x,yw)\n",
    "        scores=tf.reshape(scores,[x.shape[0],-1])\n",
    "        return tf.add(scores,x_mask)\n",
    "    \n",
    "    \n",
    "    def p_q_aligned(self):\n",
    "        question=tf.nn.embedding_lookup(self.word_embedding,self.question)\n",
    "        paragraph=tf.nn.embedding_lookup(self.word_embedding,self.paragraph)\n",
    "        q_shape=question.shape\n",
    "        p_shape=paragraph.shape\n",
    "        w1=tf.get_variable('p_q_aligned_w1',shape=[q_shape[-1],q_shape[-1]])\n",
    "        w2=tf.get_variable('p_q_aligned_w2',shape=[p_shape[-1],p_shape[-1]])\n",
    "        question=tf.nn.relu(tf.reshape(tf.matmul(tf.reshape(question,[-1,question.shape[-1]]),w1),[q_shape[0],-1,q_shape[-1]]))\n",
    "        paragraph=tf.nn.relu(tf.reshape(tf.matmul(tf.reshape(paragraph,[-1,paragraph.shape[-1]]),w2),[p_shape[0],-1,p_shape[-1]]))\n",
    "        scores=tf.matmul(paragraph,tf.transpose(question,perm=[0,2,1]))\n",
    "#         scores=tf.add(scores,self.question_mask)\n",
    "        scores=tf.nn.softmax(scores)\n",
    "        scores=tf.matmul(scores,question)\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self):\n",
    "        pre_s=tf.nn.softmax(self.logic[0])\n",
    "        pre_e=tf.nn.softmax(self.logic[1])\n",
    "        scores=tf.matmul(tf.expand_dims(pre_s,-1),tf.expand_dims(pre_e,1))\n",
    "        scores_=tf.reshape(scores,[pre_s.shape[0],-1])\n",
    "        index=tf.argmax(scores_,axis=-1)\n",
    "#         pre_s=tf.div(index,pre_s.shape[1])\n",
    "#         pre_e=tf.mod(index,pre_s.shape[1])\n",
    "        return index,scores\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data():\n",
    "    def __init__(self,train_fname,val_fname,vocab_size,word_embedding_fname,word_embedding_size):\n",
    "        self.counter=Counter()\n",
    "        self.vocabulary=['**unknow**','**null**']\n",
    "        self.feature_dict={}\n",
    "        self.vocab_size=vocab_size\n",
    "        self.word_vecs={}\n",
    "        self.word_embedding_size=word_embedding_size\n",
    "        self.data_index={'train':0,\n",
    "                        'val':0}\n",
    "        self.word_to_idx={}\n",
    "        self.create(train_fname,val_fname,vocab_size,word_embedding_fname)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def load_data(self, filename):\n",
    "        \"\"\"Load examples from preprocessed file.\n",
    "        One example per line, JSON encoded.\n",
    "        \"\"\"\n",
    "        print('load data from %s\\n' % filename)\n",
    "        # Load JSON lines\n",
    "        with open(filename) as f:\n",
    "            examples = [eval(line) for line in f]\n",
    "        for ex in tqdm.tqdm(examples):\n",
    "            ex['question'] = [w.lower() for w in ex['question']]\n",
    "            self.counter.update(ex['question'])\n",
    "            ex['document'] = [w.lower() for w in ex['document']]\n",
    "            self.counter.update(ex['document'])\n",
    "        return examples\n",
    "  \n",
    "\n",
    "    def build_feature_dict(self,examples):\n",
    "        \"\"\"Index features (one hot) from fields in examples and options.\"\"\"\n",
    "        print('build_feature_dict.............\\n')\n",
    "        def _insert(feature):\n",
    "            if feature not in self.feature_dict:\n",
    "                self.feature_dict[feature] = len(self.feature_dict)\n",
    "        \n",
    "        # Exact match features\n",
    "        _insert('in_question_uncased')\n",
    "        \n",
    "        # Part of speech tag features \n",
    "        for ex in examples:\n",
    "            for w in ex['pos']:\n",
    "                _insert('pos=%s' % w)\n",
    "\n",
    "        # Named entity tag features\n",
    "        for ex in examples:\n",
    "            for w in ex['ner']:\n",
    "                _insert('ner=%s' % w)\n",
    "\n",
    "        # Term frequency feature\n",
    "        _insert('tf')\n",
    "        print('build_feature_dict:{}'.format(self.feature_dict))\n",
    "      \n",
    "    def get_vocabulary(self,vocab_size):\n",
    "        vocab_counter=self.counter.most_common()\n",
    "        v_size=vocab_size-2\n",
    "        assert v_size<=len(vocab_counter), 'vocabulary_size:%d is too large' % vocab_size\n",
    "        v_count=0.0\n",
    "        \n",
    "        for i in range(v_size):\n",
    "            self.vocabulary.append(vocab_counter[i][0])\n",
    "            v_count+=vocab_counter[i][1]\n",
    "        v_all=v_count\n",
    "        for i in range(v_size,len(vocab_counter)):\n",
    "            v_all+=vocab_counter[i][1]\n",
    "        print('vocabulary of the total ratio is %f' % (v_count/v_all))    \n",
    "            \n",
    "    def load_word_embedding(self,fname):\n",
    "        if os.path.isfile(fname):\n",
    "            print('loading word_embedding form {}'.format(fname))\n",
    "            with open(fname) as f:\n",
    "                for line in tqdm.tqdm(f):\n",
    "                    line_values=line.strip().split()\n",
    "                    word=line_values[0].lower()\n",
    "                    assert self.word_embedding_size==len(line_values[1:]), '%s embedding_size!=self.word_embedding_size' % fname\n",
    "                    if word in self.vocabulary:\n",
    "                        self.word_vecs[word]=np.array(line_values[1:],dtype='float32')\n",
    "   \n",
    "    def add_word_embedding(self,fname):\n",
    "        print('add word_embedding not in %s' % fname)\n",
    "        for word in tqdm.tqdm(self.vocabulary):\n",
    "            if word not in self.word_vecs:\n",
    "                self.word_vecs[word]=np.random.uniform(-1,1,self.word_embedding_size)\n",
    "                    \n",
    "    def get_word_embedding(self):\n",
    "        self.word_embedding=np.zeros(shape=(self.vocab_size,self.word_embedding_size),dtype='float32')\n",
    "        for i in range(len(self.vocabulary)):\n",
    "            self.word_embedding[i]=self.word_vecs[self.vocabulary[i]]\n",
    "            \n",
    "    \n",
    "    def add_word_to_idx(self):\n",
    "        for i in range(len(self.vocabulary)):\n",
    "            self.word_to_idx[self.vocabulary[i]]=i\n",
    "    \n",
    "    \n",
    "    def get_idx_from_sent(self,sent):\n",
    "        x=[]\n",
    "        for word in sent:\n",
    "            if word in self.word_to_idx:\n",
    "                x.append(self.word_to_idx[word])\n",
    "            else:\n",
    "                x.append(self.word_to_idx['**unknow**'])\n",
    "        return x\n",
    "    \n",
    "    def vectorize(self,ex):\n",
    "        \"\"\"Torchify a single example.\"\"\"\n",
    "\n",
    "        # Index words\n",
    "        document = self.get_idx_from_sent(ex['document'])\n",
    "        question = self.get_idx_from_sent(ex['question'])\n",
    "\n",
    "        # Create extra features vector\n",
    "        \n",
    "        features = np.zeros([len(ex['document']),len(self.feature_dict)])\n",
    "        \n",
    "\n",
    "        # f_{exact_match}\n",
    "        for i in range(len(ex['document'])):\n",
    "            if ex['document'][i] in ex['question']:\n",
    "                features[i][self.feature_dict['in_question_uncased']] = 1.0\n",
    "                \n",
    "        # f_{token} (POS)\n",
    "        for i, w in enumerate(ex['pos']):\n",
    "            f = 'pos=%s' % w\n",
    "            if f in self.feature_dict:\n",
    "                features[i][self.feature_dict[f]] = 1.0\n",
    "\n",
    "        # f_{token} (NER)\n",
    "        for i, w in enumerate(ex['ner']):\n",
    "            f = 'ner=%s' % w\n",
    "            if f in self.feature_dict:\n",
    "                features[i][self.feature_dict[f]] = 1.0\n",
    "\n",
    "        # f_{token} (TF)\n",
    "        counter = Counter(ex['document'])\n",
    "        l = len(ex['document'])\n",
    "        for i, w in enumerate(ex['document']):\n",
    "            features[i][self.feature_dict['tf']] = counter[w] * 1.0 / l\n",
    "\n",
    "        # Maybe return without target\n",
    "        if 'answers' not in ex:\n",
    "            return document, features, question, ex['id']\n",
    "\n",
    "        \n",
    "        start = ex['answers'][0][0]\n",
    "        end = ex['answers'][0][1]\n",
    "\n",
    "        return document, features, question, start, end, ex['id']\n",
    "    \n",
    "    \n",
    "    def process_batch(self,batch):\n",
    "        \"\"\"Gather a batch of individual examples into one batch.\"\"\"\n",
    "        NUM_INPUTS = 3\n",
    "        NUM_TARGETS = 2\n",
    "        NUM_EXTRA = 1\n",
    "\n",
    "        ids = [ex[-1] for ex in batch]\n",
    "        docs = [ex[0] for ex in batch]\n",
    "        features = [ex[1] for ex in batch]\n",
    "        questions = [ex[2] for ex in batch]\n",
    "\n",
    "        # Batch documents and features\n",
    "        docs_len=[len(d) for d in docs]\n",
    "        max_length = max(docs_len)\n",
    "        x1 = np.ones([len(batch),max_length])\n",
    "        x1_mask = np.full([len(batch), max_length],-np.inf)\n",
    "        x1_f = np.zeros([len(batch), max_length, features[0].shape[1]])\n",
    "        for i, d in enumerate(docs):\n",
    "            x1[i, :len(d)]=d\n",
    "            x1_mask[i, :len(d)]=[0]*len(d)\n",
    "            for j in range(len(d)):\n",
    "                x1_f[i,j,:]=features[i][j]\n",
    "\n",
    "        # Batch questions\n",
    "        ques_len=[len(q) for q in questions]\n",
    "        max_length = max(ques_len)\n",
    "        x2 = np.ones([len(questions), max_length])\n",
    "        x2_mask = np.full([len(questions), max_length],-np.inf)\n",
    "        for i, q in enumerate(questions):\n",
    "            x2[i, :len(q)]=q\n",
    "            x2_mask[i, :len(q)]=[0]*len(q)\n",
    "\n",
    "        # Maybe return without targets\n",
    "        if len(batch[0]) == NUM_INPUTS + NUM_EXTRA:\n",
    "            return x1, x1_f, x1_mask, x2, x2_mask, ids\n",
    "\n",
    "        elif len(batch[0]) == NUM_INPUTS + NUM_EXTRA + NUM_TARGETS:\n",
    "            y_s = [ex[3] for ex in batch]\n",
    "            y_e = [ex[4] for ex in batch]\n",
    "        else:\n",
    "            raise RuntimeError('Incorrect number of inputs per example.')\n",
    "\n",
    "        return x1,x1_f,docs_len,x1_mask,x2,ques_len,x2_mask,y_s,y_e,ids\n",
    "        \n",
    "        \n",
    "    def process_data(self,examples):\n",
    "        dataset=[self.vectorize(ex) for ex in examples]\n",
    "        return dataset\n",
    "    \n",
    "    def get_next_batch(self,dataset,batch_size,data_class):\n",
    "        data_len=len(dataset)\n",
    "        batch_num=int(data_len/batch_size)\n",
    "        if self.data_index[data_class]==batch_num:\n",
    "            self.data_index[data_class]=0\n",
    "        c_index=self.data_index[data_class]\n",
    "        batch_data=dataset[c_index*batch_size:(c_index+1)*batch_size]\n",
    "        self.data_index[data_class]+=1\n",
    "        return self.process_batch(batch_data)\n",
    "             \n",
    "    def create(self,train_fname,val_fname,vocab_size,word_embedding_fname):\n",
    "        examples=self.load_data(train_fname)\n",
    "        self.build_feature_dict(examples)\n",
    "        self.get_vocabulary(vocab_size)\n",
    "        self.load_word_embedding(word_embedding_fname)\n",
    "        self.add_word_embedding(word_embedding_fname)\n",
    "        self.add_word_to_idx()\n",
    "        self.train_data=self.process_data(examples)\n",
    "        examples=self.load_data(val_fname)\n",
    "        self.val_data=self.process_data(examples)\n",
    "        self.get_word_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data=Data('datas/train.json','datas/valid.json',40000,'datas/word_embeddings_64.txt',64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_file=open('datas/data','rb')\n",
    "data=pickle.load(data_file)\n",
    "x=data.get_next_batch(data.train_data,2,'train')\n",
    "for i in x:\n",
    "    try:\n",
    "        print(i.shape)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "import pickle\n",
    "data_file=open('datas/data','rb')\n",
    "data=pickle.load(data_file)\n",
    "config=Config(data.word_embedding)\n",
    "epoch=50\n",
    "model=DocReader(config)\n",
    "batch_num=int(len(data.train_data)/config.batch_size)\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "loss_summary=tf.summary.scalar('loss',model.loss)\n",
    "print('batch_num:%d' % batch_num)\n",
    "def get_train_feed_dict(model,data):\n",
    "    batch_data=data.get_next_batch(data.train_data,config.batch_size,'train')\n",
    "    return {model.paragraph:batch_data[0],\n",
    "            model.paragraph_length:batch_data[2],\n",
    "            model.paragraph_mask:batch_data[3],\n",
    "            model.paragraph_features:batch_data[1],\n",
    "            model.question:batch_data[4],\n",
    "            model.question_length:batch_data[5],\n",
    "            model.question_mask:batch_data[6],\n",
    "            model.start_index:batch_data[7],\n",
    "            model.end_index:batch_data[8]}\n",
    "with tf.Session() as sess:\n",
    "    writer=tf.summary.FileWriter('logs',sess.graph)\n",
    "    sess.run(init)\n",
    "#     saver.restore(sess,'../models/model')\n",
    "    for i in range(epoch):\n",
    "        print('epoch:%d' % i)\n",
    "        for j in tqdm.tqdm(range(batch_num)):\n",
    "            feed_dict=get_train_feed_dict(model,data)\n",
    "            loss_str,_=sess.run([loss_summary,model.train],feed_dict=feed_dict)\n",
    "            writer.add_summary(loss_str,i*batch_num+j)\n",
    "#             print(sess.run(model.loss,feed_dict=feed_dict),'------------------------')\n",
    "        save_path='models/model_'+str(i)       \n",
    "        save_path=saver.save(sess,save_path)\n",
    "        print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num:626\n",
      "INFO:tensorflow:Restoring parameters from models/model_49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [01:20<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_answer(index1,index2,p):\n",
    "    if(index1>index2):\n",
    "        return 'unknow'\n",
    "    else:\n",
    "        return ''.join(p[index1:index2+1])\n",
    "\n",
    "def val(data,model,sess,config):\n",
    "    result=[]\n",
    "    batch_num=int(len(data.val_data)/config.batch_size)\n",
    "    for _ in tqdm.tqdm(range(batch_num)):\n",
    "        batch_data=data.get_next_batch(data.val_data,config.batch_size,'val')\n",
    "        feed_dict={model.paragraph:batch_data[0],\n",
    "                   model.paragraph_length:batch_data[2],\n",
    "                   model.paragraph_mask:batch_data[3],\n",
    "                   model.paragraph_features:batch_data[1],\n",
    "                   model.question:batch_data[4],\n",
    "                   model.question_length:batch_data[5],\n",
    "                   model.question_mask:batch_data[6],\n",
    "                   model.start_index:batch_data[7],\n",
    "                   model.end_index:batch_data[8]}\n",
    "        index,scores=sess.run(model.predict,feed_dict=feed_dict)\n",
    "        pre_s=sess.run(tf.div(index,scores.shape[1]))\n",
    "        pre_e=sess.run(tf.mod(index,scores.shape[-1]))\n",
    "        for i in range(config.batch_size):\n",
    "            id=batch_data[-1][i]\n",
    "            p_ind=np.asarray(batch_data[0][i],dtype='int32')\n",
    "            paragraph=[data.vocabulary[j] for j in p_ind]\n",
    "            answer=get_answer(pre_s[i],pre_e[i],paragraph)\n",
    "            result.append((id,answer,scores[i][pre_s[i]][pre_e[i]]))\n",
    "    return result\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "tf.reset_default_graph()\n",
    "import pickle\n",
    "data_file=open('datas/data','rb')\n",
    "data=pickle.load(data_file)\n",
    "config=Config(data.word_embedding)\n",
    "epoch=50\n",
    "model=DocReader(config)\n",
    "batch_num=int(len(data.train_data)/config.batch_size)\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "loss_summary=tf.summary.scalar('loss',model.loss)\n",
    "print('batch_num:%d' % batch_num)\n",
    "def get_train_feed_dict(model,data,dataset):\n",
    "    batch_data=data.get_next_batch(dataset,config.batch_size,'train')\n",
    "    return {model.paragraph:batch_data[0],\n",
    "            model.paragraph_length:batch_data[2],\n",
    "            model.paragraph_mask:batch_data[3],\n",
    "            model.paragraph_features:batch_data[1],\n",
    "            model.question:batch_data[4],\n",
    "            model.question_length:batch_data[5],\n",
    "            model.question_mask:batch_data[6],\n",
    "            model.start_index:batch_data[7],\n",
    "            model.end_index:batch_data[8]}\n",
    "with tf.Session() as sess:\n",
    "    writer=tf.summary.FileWriter('logs',sess.graph)\n",
    "    sess.run(init)\n",
    "    saver.restore(sess,'models/model_49')\n",
    "#     feed_dict=get_train_feed_dict(model,data,data.train_data)\n",
    "#     logic=sess.run(model.logic,feed_dict=feed_dict)\n",
    "#     pre_s=sess.run(tf.nn.softmax(logic[0]))\n",
    "#     pre_e=sess.run(tf.nn.softmax(logic[1]))\n",
    "    result=val(data,model,sess,config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('15000', '特殊教育学校', 0.99951774)\n",
      "0.999518\n"
     ]
    }
   ],
   "source": [
    "print(result[0])\n",
    "print(result[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict={}\n",
    "for i in result:\n",
    "    id=str(i[0])\n",
    "    if id in result_dict:\n",
    "        if result_dict[id][0]<i[-1]:\n",
    "            result_dict[id]=[i[-1],str(i[1])]\n",
    "    else:\n",
    "        result_dict[id]=[i[-1],str(i[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "for i in result_dict:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datas/val_predict_.txt','w') as f:\n",
    "    for i in result_dict:\n",
    "        line=str(i)+'\\t'+str(result_dict[i][1])+'\\n'\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p0=np.asarray(feed_dict[model.paragraph][0],dtype='int32')\n",
    "print([data.vocabulary[i] for i in p0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_s.shape)\n",
    "print(type(pre_s))\n",
    "with tf.Session() as s:\n",
    "    pre_s=tf.expand_dims(pre_s,-1)\n",
    "    pre_e=tf.expand_dims(pre_e,1)\n",
    "    scores_=s.run(tf.matmul(pre_s,pre_e))\n",
    "    scores=s.run(tf.reshape(tf.matmul(pre_s,pre_e),(pre_s.shape[0],-1)))\n",
    "    print(pre_s.shape)\n",
    "    print(pre_e.shape)\n",
    "#     print(s.run(scores))\n",
    "    \n",
    "    index=tf.argmax(scores,axis=-1)\n",
    "    print(index.shape)\n",
    "    print(s.run(index))\n",
    "    print(s.run(tf.div(index,pre_s.shape[1])))\n",
    "    print(s.run(tf.mod(index,pre_s.shape[1])))\n",
    "    print(scores_[0][12][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.constant([[[1,3,2],[4,6,5]],[[7,8,9],[10,11,12]]])\n",
    "print(x.shape)\n",
    "x_=tf.reshape(x,(x.shape[0],-1))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.argmax(x_,axis=-1)))\n",
    "    print(sess.run(tf.argmax(x,axis=1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def decode(score_s, score_e, top_n=1, max_len=None):\n",
    "        \"\"\"Take argmax of constrained score_s * score_e.\n",
    "\n",
    "        Args:\n",
    "            score_s: independent start predictions\n",
    "            score_e: independent end predictions\n",
    "            top_n: number of top scored pairs to take\n",
    "            max_len: max span length to consider\n",
    "        \"\"\"\n",
    "        pred_s = []\n",
    "        pred_e = []\n",
    "        pred_score = []\n",
    "        max_len = max_len or score_s.size(1)\n",
    "        for i in range(score_s.size(0)):\n",
    "            # Outer product of scores to get full p_s * p_e matrix\n",
    "            scores = torch.ger(score_s[i], score_e[i])\n",
    "\n",
    "            # Zero out negative length and over-length span scores\n",
    "            scores.triu_().tril_(max_len - 1)\n",
    "\n",
    "            # Take argmax or top n\n",
    "            scores = scores.numpy()\n",
    "            scores_flat = scores.flatten()\n",
    "            if top_n == 1:\n",
    "                idx_sort = [np.argmax(scores_flat)]\n",
    "            elif len(scores_flat) < top_n:\n",
    "                idx_sort = np.argsort(-scores_flat)\n",
    "            else:\n",
    "                idx = np.argpartition(-scores_flat, top_n)[0:top_n]\n",
    "                idx_sort = idx[np.argsort(-scores_flat[idx])]\n",
    "            s_idx, e_idx = np.unravel_index(idx_sort, scores.shape)\n",
    "            pred_s.append(s_idx)\n",
    "            pred_e.append(e_idx)\n",
    "            pred_score.append(scores_flat[idx_sort])\n",
    "        return pred_s, pred_e, pred_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "import pickle\n",
    "data_file=open('datas/data','rb')\n",
    "data=pickle.load(data_file)\n",
    "config=Config(data.word_embedding)\n",
    "epoch=50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess=tf.Sesssion() \n",
    "batch_num=int(len(data.train_data)/config.batch_size)\n",
    "init=tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "loss_summary=tf.summary.scalar('loss',model.loss)\n",
    "\n",
    "\n",
    "def get_train_feed_dict(model,data):\n",
    "    batch_data=data.get_next_batch(data.train_data,config.batch_size,'train')\n",
    "    return {paragraph:batch_data[0],\n",
    "            paragraph_length:batch_data[2],\n",
    "            paragraph_mask:batch_data[3],\n",
    "            paragraph_features:batch_data[1],\n",
    "            question:batch_data[4],\n",
    "            question_length:batch_data[5],\n",
    "            question_mask:batch_data[6],\n",
    "            start_index:batch_data[7],\n",
    "            end_index:batch_data[8]}\n",
    "writer=tf.summary.FileWriter('../logs',sess.graph)\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in tqdm.tqdm(range(batch_num)):\n",
    "        feed_dict=get_train_feed_dict(model,data)\n",
    "        loss_str,_=sess.run([loss_summary,model.train],feed_dict=feed_dict)\n",
    "        writer.add_summary(loss_str,i*batch_num+j)\n",
    "        if (j+1)%500==0:\n",
    "                point=str(i)+'_'+str(j)\n",
    "                save_path=saver.save(sess,'../models/model_%s' % point)\n",
    "                print(\"Model saved in path: %s\" % save_path)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "paragraph=tf.placeholder(dtype=tf.int32,shape=[config.batch_size,None])\n",
    "paragraph_length=tf.placeholder(dtype=tf.int32,shape=[config.batch_size])\n",
    "paragraph_mask=tf.placeholder(dtype=tf.float32,shape=[config.batch_size,None])\n",
    "paragraph_features=tf.placeholder(dtype=tf.float32,shape=[config.batch_size,None,None])\n",
    "question=tf.placeholder(dtype=tf.int32,shape=[config.batch_size,None])\n",
    "question_length=tf.placeholder(dtype=tf.int32,shape=[config.batch_size])\n",
    "question_mask=tf.placeholder(dtype=tf.float32,shape=[config.batch_size,None])\n",
    "start_index=tf.placeholder(dtype=tf.int32,shape=[config.batch_size])\n",
    "end_index=tf.placeholder(dtype=tf.int32,shape=[config.batch_size])\n",
    "    \n",
    "\n",
    "word_embedding=tf.get_variable('word_embedding',initializer=config.word_embedding)\n",
    "        \n",
    "        \n",
    "def get_logic():\n",
    "    p_encoding=paragraph_encoding()\n",
    "    q_encoding=question_encoding()\n",
    "    start_scores=biliner_attention(p_encoding,q_encoding,paragraph_mask)\n",
    "    end_scores=biliner_attention(p_encoding,q_encoding,paragraph_mask)\n",
    "    return start_scores,end_scores\n",
    "        \n",
    "        \n",
    "def get_loss(start_scores,end_scores):\n",
    "    loss=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=start_index,logits=start_scores)+\\\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=end_index,logits=end_scores)\n",
    "    loss=tf.reduce_mean(loss)\n",
    "    return loss\n",
    "        \n",
    "        \n",
    "def train(loss):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op=optimizer.minimize(loss)\n",
    "    return train_op\n",
    "    \n",
    "    \n",
    "def paragraph_encoding():\n",
    "    def get_cell():\n",
    "        cell=tf.nn.rnn_cell.BasicLSTMCell(config.paragraph_encoding_hidden_size)\n",
    "        cell=tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=self.config.dropout)\n",
    "        return cell\n",
    "    \n",
    "    cell_fw=tf.nn.rnn_cell.MultiRNNCell([get_cell() for _ in range(self.config.paragraph_encoding_layer_num)])\n",
    "    cell_bw=tf.nn.rnn_cell.MultiRNNCell([get_cell() for _ in range(self.config.paragraph_encoding_layer_num)])\n",
    "    paragraph=tf.nn.embedding_lookup(word_embedding,paragraph)\n",
    "    print(paragraph)\n",
    "    p_encoding,_=tf.nn.bidirectional_dynamic_rnn(cell_fw,cell_bw,paragraph,sequence_length=self.paragraph_length,dtype=tf.float32)\n",
    "#     p_encoding,_=tf.nn.bidirectional_dynamic_rnn(cell,cell,paragraph,sequence_length=paragraph_length,dtype=tf.float32)\n",
    "\n",
    "    p_encoding=tf.concat(p_encoding,-1)\n",
    "    p_es=[p_encoding]\n",
    "    p_es.append(paragraph_features)\n",
    "    p_q_scores=p_q_aligned()\n",
    "    p_es.append(p_q_scores)\n",
    "    p_encoding=tf.concat(p_es,-1)\n",
    "    return p_encoding\n",
    "    \n",
    "def question_encoding():\n",
    "    cell=tf.nn.rnn_cell.BasicLSTMCell(config.question_encoding_hidden_size)\n",
    "    cell=tf.nn.rnn_cell.DropoutWrapper(cell,output_keep_prob=config.dropout)\n",
    "    cell_fw=tf.nn.rnn_cell.MultiRNNCell([cell]*config.question_encoding_layer_num)\n",
    "    cell_bw=tf.nn.rnn_cell.MultiRNNCell([cell]*config.question_encoding_layer_num)\n",
    "    question=tf.nn.embedding_lookup(word_embedding,question)\n",
    "    q_encoding,_=tf.nn.bidirectional_dynamic_rnn(cell_fw,cell_bw,question,sequence_length=question_length,dtype=tf.float32)\n",
    "    q_encoding=tf.concat(q_encoding,-1)\n",
    "    alpha=liner_attention(q_encoding,question_mask)\n",
    "    alpha=tf.cast([tf.expand_dims(alpha,-1)]*config.question_encoding_hidden_size*2,-1)\n",
    "    q_encoding=tf.reduce_sum(tf.multiply(q_encoding,alpha),axis=1)\n",
    "    return q_encoding\n",
    "    \n",
    "def liner_attention(x,x_mask):\n",
    "    w=tf.get_variable('liner_attention_w',shape=[x.shape[-1],1])\n",
    "    scores=tf.matmul(tf.reshape(x,(-1,x.shape[-1])),w)\n",
    "    scores=tf.add(tf.reshape(scores,(x.shape[0],x.shape[1])),x_mask)\n",
    "    alpha=tf.nn.softmax(scores)\n",
    "    return alpha\n",
    "    \n",
    "def biliner_attention(x,y,x_mask):\n",
    "    w=tf.get_variable('biliner_attention_w',shape=[y.shape[-1],x.shape[-1]])\n",
    "    yw=tf.matmul(y,w)\n",
    "    scores=[]\n",
    "    for i in range(x.shape[0]):\n",
    "        scores_i=[]\n",
    "        for j in range(x.shape[1]):\n",
    "            scores_i.append(tf.reduce_sum(tf.multiply(yw[i],x[i][j])))\n",
    "    return tf.add(scores,x_mask)\n",
    "    \n",
    "    \n",
    "def p_q_aligned():\n",
    "    question=tf.nn.embedding_lookup(word_embedding,question)\n",
    "    paragraph=tf.nn.embedding_lookup(self.word_embedding,self.paragraph)\n",
    "    q_shape= sess.run(question,feed_dict={question:}).shape\n",
    "        p_shape=paragraph.eval(self.sess).shape\n",
    "        w1=tf.get_variable('p_q_aligned_w1',shape=[q_shape[-1],q_shape[-1]])\n",
    "        w2=tf.get_variable('p_q_aligned_w2',shape=[p_shape[-1],p_shape[-1]])\n",
    "        question=tf.nn.relu(tf.reshape(tf.matmul(tf.reshape(question,[-1,question.shape[-1]]),w1),[q_shape[0],-1,q_shape[-1]]))\n",
    "        paragraph=tf.nn.relu(tf.reshape(tf.matmul(tf.reshape(paragraph,[-1,paragraph.shape[-1]]),w2),[p_shape[0],-1,p_shape[-1]]))\n",
    "        scores=[]\n",
    "        for i in range(p_shape[0]):\n",
    "            scores_i=[]\n",
    "            for j in range(p_shape[1]):\n",
    "                scores_i_j=[]\n",
    "                for k in range(q_shape[1]):\n",
    "                    score=tf.add(tf.reduce_sum(tf.multiply(paragraph[i][j],question[i][k])),\\\n",
    "                                 self.question_mask[i][k])\n",
    "                    scores_i_j.append(score)\n",
    "                scores_i.append(scores_i_j)\n",
    "            scores.append(scores_i)\n",
    "        scores=tf.nn.softmax(scores)\n",
    "        return scores\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)\n",
    "# print(x[0])\n",
    "# print(x[1])\n",
    "# print(x[2])\n",
    "# print(x[7])\n",
    "# print(data.train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "data_file=open('datas/data','wb')\n",
    "pickle.dump(data,data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[[1,2],[3,4]]\n",
    "z=tf.constant([1,2,4])\n",
    "# z=tf.reshape(z,[1,-1,1])\n",
    "# print(z.shape)\n",
    "# z=tf.reduce_sum(z)\n",
    "z[0]=3\n",
    "# for i in range(tf.c(z[0])):\n",
    "#     print(i)\n",
    "y=[[[1,1],[1,1]],[[2,2],[2,2]]]\n",
    "r=tf.multiply(tf.concat([tf.expand_dims(x,-1)]*2,-1),y)\n",
    "with tf.Session() as s:\n",
    "#     print(s.run(x))\n",
    "#     print(s.run(r))\n",
    "#     print(s.run(tf.reduce_sum(y,axis=-1)))\n",
    "#     print(s.run(z+1))\n",
    "    print(s.run(z))\n",
    "    print(s.run(tf.transpose(z[0][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1.,1.,3.,4.,1.]\n",
    "y=[1,1,1,-float('inf'),-float('inf')]\n",
    "with tf.Session() as s:\n",
    "    print(s.run(tf.multiply(x,y)))\n",
    "    print(s.run(tf.nn.softmax(tf.multiply(x,y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(0.000000001*-float('inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.full([2,3],-np.inf)\n",
    "print(x.shape[1])\n",
    "x[0][1]=1\n",
    "x[0][:2]=[2]*2\n",
    "x[1][0]=1\n",
    "x[1,].fill(3)\n",
    "print(x)\n",
    "# x=tf.constant(x,dtype=tf.float32)\n",
    "# y=tf.fill([2,3],9.0)\n",
    "with tf.Session() as s:\n",
    "    print(s.run(tf.nn.softmax(x)))\n",
    "#     print(s.run(tf.reduce_sum(x)))\n",
    "#     print(s.run(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1+\\\n",
    "      2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('1\\n2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.zeros((2,3,4))\n",
    "x=np.ones((2,4))\n",
    "print(y)\n",
    "y[1,1,:]=x[0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x={1:1}\n",
    "x[1]+=1\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[[[1,2],[2,-np.inf]],[[1,2],[2,2]]]\n",
    "\n",
    "# y=[[[1,1],[2,2]]]\n",
    "x_=tf.placeholder(dtype=tf.float32)\n",
    "def test():\n",
    "    y=np.ones((2,3))\n",
    "    y[0][0]=x_[0][0][0]\n",
    "    return y\n",
    "with tf.Session() as s:\n",
    "#     print(s.run(x_+1,feed_dict={x_:x}))\n",
    "#     print(s.run(tf.concat([x,y],-1)))\n",
    "    print(s.run(tf.nn.softmax(x_),feed_dict={x_:x}))\n",
    "    print(s.run(test(),feed_dict={x_:x}))\n",
    "    print(s.run(x_[0][0][0],feed_dict={x_:x}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1*-np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-np.inf*-np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x=tf.constant([[[1,1,1],[2,2,2],[3,3,3]]],dtype=tf.float32)\n",
    "\n",
    "y=tf.constant([[[1,1,1],[2,2,2]]],dtype=tf.float32)\n",
    "# tf.assign()\n",
    "\n",
    "x_mask=tf.constant([[0,0,-np.inf]])\n",
    "y_mask=tf.constant([[0,-np.inf]])\n",
    "\n",
    "def p_q_aligned(question,paragraph,question_mask,paragraph_mask):\n",
    "        q_shape=question.shape\n",
    "        print(q_shape)\n",
    "        p_shape=paragraph.shape\n",
    "        print(p_shape)\n",
    "        w1=tf.get_variable('p_q_aligned_w1',shape=[q_shape[-1],q_shape[-1]])\n",
    "        w2=tf.get_variable('p_q_aligned_w2',shape=[p_shape[-1],p_shape[-1]])\n",
    "        question=tf.nn.relu(tf.reshape(tf.matmul(tf.reshape(question,[-1,question.shape[-1]]),w1),q_shape))\n",
    "        paragraph=tf.nn.relu(tf.reshape(tf.matmul(tf.reshape(paragraph,[-1,paragraph.shape[-1]]),w2),p_shape))\n",
    "        scores=[]\n",
    "        for i in range(p_shape[0]):\n",
    "            scores_i=[]\n",
    "            for j in range(p_shape[1]):\n",
    "                scores_i_j=[]\n",
    "                for k in range(q_shape[1]):\n",
    "                    score=tf.reduce_sum(tf.multiply(paragraph[i][j],question[i][k]))\n",
    "#                     score=tf.add(score,paragraph_mask[i][j])\n",
    "                    score=tf.add(score,question_mask[i][k])\n",
    "                    scores_i_j.append(score)\n",
    "                scores_i.append(scores_i_j)\n",
    "            scores.append(scores_i)\n",
    "        scores=tf.nn.softmax(scores)\n",
    "        return scores\n",
    "    \n",
    "result=p_q_aligned(x,y,x_mask,y_mask)\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as s:\n",
    "    s.run(init)\n",
    "    print(s.run(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([1,2,3,4,5])\n",
    "y=np.array([1,2,3,4,5])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[[[1,2,3],[1,2,3]]]\n",
    "with tf.Session() as s:\n",
    "    print(s.run(tf.reduce_sum(x,axis=1)))\n",
    "    print(s.run(tf.nn.softmax([1,np.inf])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1,2]\n",
    "print(x[1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(1)+'_'+str(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Create input data\n",
    "X = np.random.randn(2, 10, 8)\n",
    "\n",
    "# The second example is of length 6 \n",
    "X[1,6,:] = 0\n",
    "X_lengths = [10, 6]\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)\n",
    "cell = tf.nn.rnn_cell.DropoutWrapper(cell=cell, output_keep_prob=0.5)\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell(cells=[cell] * 4, state_is_tuple=True)\n",
    "\n",
    "outputs, last_states = tf.nn.dynamic_rnn(\n",
    "    cell=cell,\n",
    "    dtype=tf.float64,\n",
    "    sequence_length=X_lengths,\n",
    "    inputs=X)\n",
    "\n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\"outputs\": outputs, \"last_states\": last_states},\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "\n",
    "\n",
    "print(result[0][\"outputs\"].shape)\n",
    "print(result[0][\"outputs\"])\n",
    "assert result[0][\"outputs\"].shape == (2, 10, 64)\n",
    "\n",
    "# Outputs for the second example past past length 6 should be 0\n",
    "assert (result[0][\"outputs\"][1,7,:] == np.zeros(cell.output_size)).all()\n",
    "\n",
    "print(result[0][\"last_states\"][0].h.shape)\n",
    "print(result[0][\"last_states\"][0].h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create input data\n",
    "X = np.random.randn(2, 10, 8)\n",
    "\n",
    "# The second example is of length 6 \n",
    "X[1,6:] = 0\n",
    "X_lengths = [10, 6]\n",
    "\n",
    "cell = tf.nn.rnn_cell.LSTMCell(num_units=64, state_is_tuple=True)\n",
    "\n",
    "outputs, states  = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=cell,\n",
    "    cell_bw=cell,\n",
    "    dtype=tf.float64,\n",
    "    sequence_length=X_lengths,\n",
    "    inputs=X)\n",
    "\n",
    "output_fw, output_bw = outputs\n",
    "states_fw, states_bw = states\n",
    "\n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\"output_fw\": output_fw, \"output_bw\": output_bw, \"states_fw\": states_fw, \"states_bw\": states_bw},\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "\n",
    "print(result[0][\"output_fw\"].shape)\n",
    "print(result[0][\"output_bw\"].shape)\n",
    "print(result[0][\"states_fw\"].h.shape)\n",
    "print(result[0][\"states_bw\"].h.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.constant([[[1,2,3]],[[1,2,3]]])\n",
    "y=tf.constant([[1,1,1],[2,2,2]])\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "with tf.Session() as s:\n",
    "    xy=tf.add(x,y)\n",
    "    print(xy.shape)\n",
    "    print(s.run(xy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "print('sdfs%d' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "  \n",
    "def getPositon():  \n",
    "    a = np.mat([[2, 5, 7, 8, 9, 89], [6, 7, 5, 4, 6, 90]])  \n",
    "      \n",
    "    raw, column = a.shape# get the matrix of a raw and column  \n",
    "      \n",
    "    _positon = np.argmax(a)# get the index of max in the a  \n",
    "    print( _positon )  \n",
    "    m, n = divmod(_positon, column)  \n",
    "    print (\"The raw is \" ,m  )\n",
    "    print (\"The column is \",  n  )\n",
    "    print (\"The max of the a is \", a[m , n])  \n",
    "  \n",
    "getPositon() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[1,2,3]\n",
    "print(x[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
